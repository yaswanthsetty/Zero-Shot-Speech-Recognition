# Changelog

All notable changes to the Zero-Shot Spoken Language Identification project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-09-26

### ðŸŽ‰ Initial Release

#### Added
- **Complete zero-shot language identification pipeline**
  - End-to-end system from audio input to language prediction
  - Support for 15 seen and 5 unseen languages in demonstration
  - Modular architecture with clear separation of concerns

- **Audio Processing Module** (`src/features.py`)
  - Wav2Vec2-based audio feature extraction
  - Support for variable-length audio inputs
  - Automatic resampling to 16kHz
  - Mean-pooled 768-dimensional embeddings

- **Phonological Feature System** (`src/features.py`)
  - Integration with Panphon library for cross-linguistic features
  - 22-dimensional phonological fingerprints
  - Language-specific phoneme inventory modeling
  - Fallback to consistent random vectors for robustness

- **Neural Architecture** (`src/model.py`)
  - ProjectionHead MLP with 667K parameters
  - Two hidden layers with ReLU activation and dropout
  - Configurable architecture parameters
  - Model checkpointing and loading functionality

- **Training Infrastructure** (`src/train.py`)
  - AdamW optimizer with learning rate scheduling
  - Cosine embedding loss for similarity learning
  - Gradient clipping for training stability
  - Comprehensive training monitoring and logging

- **Evaluation Framework** (`src/evaluate.py`)
  - Zero-shot evaluation on unseen languages
  - Top-1 and Top-3 accuracy metrics
  - Per-language performance analysis
  - Confusion matrix and similarity analysis
  - Detailed evaluation reporting

- **Data Pipeline** (`src/data_prep.py`)
  - FLEURS dataset integration (with fallback)
  - Synthetic data generation for demonstration
  - Flexible train/validation/test splitting
  - Custom PyTorch dataset and dataloader implementation

- **Configuration Management** (`src/config.py`)
  - Centralized hyperparameter configuration
  - Automatic device detection (CPU/GPU)
  - Language subset definitions
  - Easily adjustable training parameters

- **Development Environment**
  - GitHub Codespaces devcontainer configuration
  - Complete dependency management
  - Professional documentation structure

#### Technical Specifications
- **Languages**: 20 total (15 seen, 5 unseen)
- **Model Size**: 667,670 parameters
- **Audio Format**: 16kHz, up to 30 seconds
- **Feature Dimensions**: 768 (audio) â†’ 22 (phonological)
- **Training**: 10 epochs with adaptive learning rate

#### Performance Characteristics
- **Training Time**: ~15 minutes on CPU for demo dataset
- **Memory Usage**: ~2GB RAM for inference
- **Batch Processing**: Configurable batch size (default: 32)
- **Zero-shot Capability**: Tested on 3 unseen Nordic languages

#### Documentation
- Comprehensive README with usage examples
- API documentation with detailed docstrings
- Contributing guidelines for open source collaboration
- MIT License for permissive usage

#### Dependencies
- PyTorch 2.8.0+ for deep learning framework
- Transformers 4.56+ for pre-trained models
- Datasets 4.1+ for data loading (when available)
- Panphon 0.22+ for phonological features
- Librosa 0.11+ for audio processing
- Additional utilities: tqdm, numpy, pandas, scikit-learn

### ðŸ”§ Technical Implementation Details

#### Architecture Decisions
- **Modular Design**: Clear separation between data, features, model, training, and evaluation
- **Fallback Mechanisms**: Synthetic data generation when real datasets unavailable
- **Error Handling**: Comprehensive exception handling and logging
- **Reproducibility**: Fixed random seeds and deterministic operations

#### Known Limitations
- FLEURS dataset compatibility issues with newer Datasets library
- Panphon feature extraction requiring manual phoneme mappings
- Limited to CPU training in current demo (GPU support available)
- Synthetic data demonstration (real data integration pending)

#### Future Roadmap
- [ ] Real FLEURS dataset integration
- [ ] Enhanced phonological feature extraction
- [ ] Attention-based model architectures
- [ ] Multi-lingual evaluation benchmarks
- [ ] Production deployment optimizations

---

**Full Changelog**: https://github.com/yaswanthsetty/Zero-Shot-Speech-Recognition/commits/main